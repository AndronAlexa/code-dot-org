#!/usr/bin/env ruby

require_relative '../../../dashboard/config/environment'
require 'cdo/aws/s3'
require 'json'
require 'json-schema'
require 'uri'
require 'date'
require 'cdo/redshift'
require 'aws-sdk-sts'

# This script redelivers application event ('firehose') event records to Redshift from the Intermediate S3 bucket
# where batches of records were stored by the Firehose data stream and where manifest objects identify the batches
# that succeeded delivery to Redshift and those that failed delivery to Redshift.

CDO.log = Logger.new($stdout)

options = {actually_update: false}
OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename(__FILE__)} [options]"
  opts.on('-u', '--actually-update', 'Actually perform the update.') do
    options[:actually_update] = true
  end
  opts.on('-h', '--help', 'Add -u to perform the update.') do
    puts opts
    exit
  end
end.parse!
CDO.log.info "Called with options: #{options}"

# To speed up the queries, we limit the query to an id range that we know covers
# all possible affected users
START_DATE_TIME = 71_826_397
END_DATE_TIME = 71_929_059

# batches_delivered = 0
# batches_failed = 0
# records_delivered = 0
# records_failed = 0

FIREHOSE_STREAM = 'analysis-events'.freeze
FIREHOSE_INTERMEDIATE_BUCKET = 'firehose-analysis-events'.freeze
ERROR_MANIFESTS_PREFIX = 'errors/manifests/2024/07/14'.freeze
REDELIVER_DATA_PREFIX = 'redeliveries'.freeze
REDELIVERY_MANIFESTS_PREFIX = REDELIVER_DATA_PREFIX + '/manifests'
REDELIVERY_ERROR_MANIFESTS_PREFIX = REDELIVER_DATA_PREFIX + '/errors'
REDSHIFT_SCHEMA = 'analysis'.freeze
REDSHIFT_TABLE = 'application_events'.freeze
sts_client = Aws::STS::Client.new
ACCOUNT_ID = sts_client.get_caller_identity.account

def split_json_batch(batch_string)
  batch_string.delete_prefix('{').
    delete_suffix('}').
    split('}{').
    map {|obj| JSON.parse("{#{obj}}")}
end

event_json_schema = {
  "type" => "object",
  "properties" => {
    "created_at" => {
      "type" => "string",
      "format" => "date-time"
    },
    "environment" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study_group" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "device" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 1024
    },
    "uuid" => {
      "type" => ["string", "null"],
      "maxLength" => 128
    },
    "user_id" => {
      "type" => ["integer", "null"]
    },
    "script_id" => {
      "type" => ["integer", "null"]
    },
    "level_id" => {
      "type" => ["integer", "null"]
    },
    "project_id" => {
      "type" => ["string", "integer", "null"],
      "maxLength" => 128
    },
    "event" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "data_int" => {
      "type" => ["integer", "null"]
    },
    "data_float" => {
      "type" => ["number", "null"]
    },
    "data_string" => {
      "type" => ["string", "null"],
      "maxLength" => 4096
    },
    "data_json" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 65535
    }
  },
  "required" => ["created_at", "environment", "study", "event"]
}

class RecordValidationError < StandardError
  attr_reader :errors

  def initialize(errors)
    @errors = errors
    super("Record validation failed: #{errors.join(', ')}")
  end
end

def validate_record(record, schema, truncate: false)
  errors = []

  # Validate required fields
  schema['required'].each do |field|
    next unless !record.key?(field) || record[field].nil?
    error_msg = "Missing required field: #{field}"
    errors << error_msg
    CDO.log.error(error_msg)
  end

  # Validate each property
  schema['properties'].each do |field, field_schema|
    next unless record.key?(field) && !record[field].nil?
    value = record[field]

    # Type validation
    types = Array(field_schema['type'])
    unless types.any? {|t| valid_type?(value, t)}
      error_msg = "Invalid type for #{field}: expected #{types.join(' or ')}, got #{value.class}"
      errors << error_msg
      CDO.log.error(error_msg)
    end

    # Format validation
    if field_schema['format'] == 'date-time' && types.include?('string')
      begin
        DateTime.iso8601(value)
      rescue ArgumentError
        error_msg = "Invalid date-time format for #{field}: #{value}"
        errors << error_msg
        CDO.log.error(error_msg)
      end
    end

    # Length validation and truncation
    next unless field_schema['maxLength'] && (types.include?('string') || types.include?('object'))
    string_value = value.is_a?(String) ? value : value.to_json
    byte_length = string_value.encode('UTF-8').bytesize
    next unless byte_length > field_schema['maxLength']
    error_msg = "#{field} exceeds maximum byte length of #{field_schema['maxLength']} (current: #{byte_length} bytes)"
    errors << error_msg
    CDO.log.error(error_msg)

    if truncate
      truncated_value = truncate_to_byte_length(string_value, field_schema['maxLength'])
      record[field] = truncated_value.is_a?(String) ? truncated_value : JSON.parse(truncated_value)
    end
  end

  raise RecordValidationError.new(errors) if errors.any? && !truncate
  record
end

def truncate_to_byte_length(string, max_bytes)
  return string if string.encode('UTF-8').bytesize <= max_bytes

  truncated = string.encode('UTF-8')
  truncated = truncated[0...-1] while truncated.bytesize > max_bytes
  truncated
end

def valid_type?(value, type)
  case type
  when 'string'
    value.is_a?(String)
  when 'integer'
    value.is_a?(Integer)
  when 'number'
    value.is_a?(Numeric)
  when 'object'
    value.is_a?(Hash)
  when 'null'
    value.nil?
  else
    false
  end
end

s3_client = AWS::S3.create_client
redshift_client = RedshiftClient.instance

continuation_token = nil

loop do
  list_error_manifests_response = s3_client.list_objects_v2(
    bucket: FIREHOSE_INTERMEDIATE_BUCKET,
    prefix: ERROR_MANIFESTS_PREFIX,
    continuation_token: continuation_token
  )

  CDO.log.info list_error_manifests_response.contents.length
  list_error_manifests_response.contents.each do |error_manifest|
    CDO.log.info error_manifest.key
    error_manifest_response = s3_client.get_object(
      bucket: FIREHOSE_INTERMEDIATE_BUCKET,
      key: error_manifest.key
    )
    error_manifest = error_manifest_response[:body].read
    CDO.log.info error_manifest
    error_manifest_json = JSON.parse(error_manifest)
    error_manifest_entries = error_manifest_json["entries"]
    error_manifest_entries.each do |entry|
      undelivered_batch_uri = URI.parse(entry["url"])
      CDO.log.info undelivered_batch_uri
      undelivered_batch_response = s3_client.get_object(
        bucket: undelivered_batch_uri.host,
        key: undelivered_batch_uri.path&.delete_prefix('/')
      )
      undelivered_batch = undelivered_batch_response.body.read
      modified_batch = ''
      undelivered_records = split_json_batch(undelivered_batch)
      undelivered_records.each do |record|
        # CDO.log.info record
        modified_record = validate_record(record, event_json_schema, truncate: true)
        modified_batch += modified_record.to_json
      rescue RecordValidationError => exception
        CDO.log.info record
        CDO.log.info exception.message
      end
      modified_batch_s3_key = REDELIVER_DATA_PREFIX + undelivered_batch_uri.path
      raise StandardError unless options[:actually_update]
      s3_client.put_object(bucket: FIREHOSE_INTERMEDIATE_BUCKET, key: modified_batch_s3_key, body: modified_batch)
      redelivery_manifest = {
        entries: [
          {
            url: "s3://#{FIREHOSE_INTERMEDIATE_BUCKET}/#{modified_batch_s3_key}",
            mandatory: true,
            meta: {
              content_length: modified_batch.encode('UTF-8').bytesize
            }
          }
        ]
      }
      load_batch_query = <<~SQL
        SET search_path TO #{REDSHIFT_SCHEMA};
        COPY #{REDSHIFT_TABLE} (created_at, environment, study, study_group, device, uuid, user_id, script_id, level_id, project_id, event, data_int, data_float, data_string, data_json)
        FROM 's3://#{FIREHOSE_INTERMEDIATE_BUCKET}/#{modified_batch_s3_key}'
        CREDENTIALS 'aws_iam_role=arn:aws:iam::#{ACCOUNT_ID}:role/redshift-s3'
        json 'auto' timeformat 'auto';
      SQL
      CDO.log.info load_batch_query
      redshift_client.exec(load_batch_query)
      s3_client.put_object(
        bucket: FIREHOSE_INTERMEDIATE_BUCKET,
        key: REDELIVERY_MANIFESTS_PREFIX + undelivered_batch_uri.path,
        body: redelivery_manifest.to_json
      )
      CDO.log.info "Successfully redelivered #{undelivered_batch_uri.path} to Redshift."
    rescue RedshiftClient::PostgreSQLQueryError => exception
      CDO.log.info "Error redelivering #{undelivered_batch_uri.path}"
      CDO.log.info exception.message
      s3_client.put_object(
        bucket: FIREHOSE_INTERMEDIATE_BUCKET,
        key: REDELIVERY_ERROR_MANIFESTS_PREFIX + undelivered_batch_uri.path,
        body: redelivery_manifest
      )
    rescue Aws::Errors::ServiceError => exception
      CDO.log.info exception.message
    end
    CDO.log.info error_manifest_json
  end
  break unless list_error_manifests_response.is_truncated
  continuation_token = list_error_manifests_response.next_continuation_token
end

CDO.log.info "Script completed"
# Log number of batches processed, number of records processed, success and failure for both.
