#!/usr/bin/env ruby

require_relative '../../../dashboard/config/environment'
require 'cdo/aws/s3'
require 'json'
require 'json-schema'
require 'uri'
require 'date'

# This script redelivers application event ('firehose') event records to Redshift from the Intermediate S3 bucket
# where batches of records were stored by the Firehose data stream and where manifest objects identify the batches
# that succeeded delivery to Redshift and those that failed delivery to Redshift.

CDO.log = Logger.new($stdout)

options = {actually_update: false}
OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename(__FILE__)} [options]"
  opts.on('-u', '--actually-update', 'Actually perform the update.') do
    options[:actually_update] = true
  end
  opts.on('-h', '--help', 'Add -u to perform the update.') do
    puts opts
    exit
  end
end.parse!
CDO.log.info "Called with options: #{options}"

# To speed up the queries, we limit the query to an id range that we know covers
# all possible affected users
START_DATE_TIME = 71_826_397
END_DATE_TIME = 71_929_059

# batches_delivered = 0
# batches_failed = 0
# records_delivered = 0
# records_failed = 0

FIREHOSE_INTERMEDIATE_BUCKET = 'firehose-analysis-events'.freeze
ERROR_MANIFESTS_PREFIX = 'errors/manifests/2024/07/14'.freeze

def split_json_batch(batch_string)
  batch_string.delete_prefix('{').
    delete_suffix('}').
    split('}{').
    map {|obj| JSON.parse("{#{obj}}")}
end

event_json_schema = {
  "type" => "object",
  "properties" => {
    "created_at" => {
      "type" => "string",
      "format" => "date-time"
    },
    "environment" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study_group" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "device" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 1024
    },
    "uuid" => {
      "type" => ["string", "null"],
      "maxLength" => 128
    },
    "user_id" => {
      "type" => ["integer", "null"]
    },
    "script_id" => {
      "type" => ["integer", "null"]
    },
    "level_id" => {
      "type" => ["integer", "null"]
    },
    "project_id" => {
      "type" => ["string", "integer", "null"],
      "maxLength" => 128
    },
    "event" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "data_int" => {
      "type" => ["integer", "null"]
    },
    "data_float" => {
      "type" => ["number", "null"]
    },
    "data_string" => {
      "type" => ["string", "null"],
      "maxLength" => 4096
    },
    "data_json" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 65535
    }
  },
  "required" => ["created_at", "environment", "study", "event"]
}

class RecordValidationError < StandardError
  attr_reader :errors

  def initialize(errors)
    @errors = errors
    super("Record validation failed: #{errors.join(', ')}")
  end
end

def validate_record(record, schema, truncate: false)
  errors = []

  # Validate required fields
  schema['required'].each do |field|
    next unless !record.key?(field) || record[field].nil?
    error_msg = "Missing required field: #{field}"
    errors << error_msg
    CDO.log.error(error_msg)
  end

  # Validate each property
  schema['properties'].each do |field, field_schema|
    next unless record.key?(field) && !record[field].nil?
    value = record[field]

    # Type validation
    types = Array(field_schema['type'])
    unless types.any? {|t| valid_type?(value, t)}
      error_msg = "Invalid type for #{field}: expected #{types.join(' or ')}, got #{value.class}"
      errors << error_msg
      CDO.log.error(error_msg)
    end

    # Format validation
    if field_schema['format'] == 'date-time' && types.include?('string')
      begin
        DateTime.iso8601(value)
      rescue ArgumentError
        error_msg = "Invalid date-time format for #{field}: #{value}"
        errors << error_msg
        CDO.log.error(error_msg)
      end
    end

    # Length validation and truncation
    next unless field_schema['maxLength'] && (types.include?('string') || types.include?('object'))
    string_value = value.is_a?(String) ? value : value.to_json
    byte_length = string_value.encode('UTF-8').bytesize
    next unless byte_length > field_schema['maxLength']
    error_msg = "#{field} exceeds maximum byte length of #{field_schema['maxLength']} (current: #{byte_length} bytes)"
    errors << error_msg
    CDO.log.error(error_msg)

    if truncate
      truncated_value = truncate_to_byte_length(string_value, field_schema['maxLength'])
      record[field] = truncated_value.is_a?(String) ? truncated_value : JSON.parse(truncated_value)
    end
  end

  raise RecordValidationError.new(errors) if errors.any? && !truncate
  record
end

def truncate_to_byte_length(string, max_bytes)
  return string if string.encode('UTF-8').bytesize <= max_bytes

  truncated = string.encode('UTF-8')
  truncated = truncated[0...-1] while truncated.bytesize > max_bytes
  truncated
end

def valid_type?(value, type)
  case type
  when 'string'
    value.is_a?(String)
  when 'integer'
    value.is_a?(Integer)
  when 'number'
    value.is_a?(Numeric)
  when 'object'
    value.is_a?(Hash)
  when 'null'
    value.nil?
  else
    false
  end
end

s3_client = AWS::S3.create_client

continuation_token = nil

loop do
  list_error_manifests_response = s3_client.list_objects_v2(
    bucket: FIREHOSE_INTERMEDIATE_BUCKET,
    prefix: ERROR_MANIFESTS_PREFIX,
    continuation_token: continuation_token
  )

  CDO.log.info list_error_manifests_response.contents.length
  list_error_manifests_response.contents.each do |error_manifest|
    CDO.log.info error_manifest.key
    error_manifest_response = s3_client.get_object(
      bucket: FIREHOSE_INTERMEDIATE_BUCKET,
      key: error_manifest.key
    )
    error_manifest = error_manifest_response[:body].read
    CDO.log.info error_manifest
    error_manifest_json = JSON.parse(error_manifest)
    error_manifest_entries = error_manifest_json["entries"]
    error_manifest_entries.each do |entry|
      undelivered_batch_uri = URI.parse(entry["url"])
      CDO.log.info undelivered_batch_uri
      undelivered_batch_response = s3_client.get_object(
        bucket: undelivered_batch_uri.host,
        key: undelivered_batch_uri.path&.delete_prefix('/')
      )
      undelivered_batch = undelivered_batch_response.body.read
      modified_batch = ''
      undelivered_records = split_json_batch(undelivered_batch)
      undelivered_records.each do |record|
        # CDO.log.info record
        modified_record = validate_record(record, event_json_schema, truncate: true)
        modified_batch += modified_record.to_json
      rescue RecordValidationError => exception
        CDO.log.info record
        CDO.log.info exception.message
      end
      File.write(undelivered_batch_uri.path.split('/').last, modified_batch)
    end
    CDO.log.info error_manifest_json
  end
  break unless list_error_manifests_response.is_truncated
  continuation_token = list_error_manifests_response.next_continuation_token
end

# LIST all error manifest files from START_DATE_TIME to END_DATE_TIME
# For each error manifest file
#   GET batch object
#   split batch object into separate JSON records
#   For each record
#     parse JSON
#     validate JSON Properties against destination Redshift table column datatypes
#     log validation errors
#     fix data (truncate Properties that exceed max length)
#     Raise TestRun Error if DRY RUN
#     queue record for re-delivery? add to a new batch file?
#     append Batch ID and record identifier to redeliver log
#     Rescue
#       log error
#       append Batch ID and record identifier to redelivery error log

CDO.log.info "Script completed"
# Log number of batches processed, number of records processed, success and failure for both.
